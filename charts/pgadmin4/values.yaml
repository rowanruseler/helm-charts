# Default values for pgAdmin4.

replicaCount: 1

## pgAdmin4 container image
##
image:
  registry: docker.io
  repository: dpage/pgadmin4
  tag: "4.29"
  pullPolicy: IfNotPresent

## Deployment annotations
annotations: {}

service:
  type: ClusterIP
  port: 80
  targetPort: http
  # targetPort: 4181 To be used with a proxy extraContainer
  portName: http

  annotations: {}
    ## Special annotations at the service level, e.g
    ## this will set vnet internal IP's rather than public ip's
    ## service.beta.kubernetes.io/azure-load-balancer-internal: "true"

  ## Specify the nodePort value for the service types.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
  ##
  # nodePort:

## Strategy used to replace old Pods by new ones
## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
##
strategy: {}
  # type: RollingUpdate
  # rollingUpdate:
  #   maxSurge: 0
  #   maxUnavailable: 1

## Server definitions will be loaded at launch time. This allows connection
## information to be pre-loaded into the instance of pgAdmin4 in the container.
## Ref: https://www.pgadmin.org/docs/pgadmin4/latest/import_export_servers.html
##
serverDefinitions:
  ## If true, server definitions will be created
  ##
  enabled: false

  servers: |-
  #  "1": {
  #    "Name": "Minimally Defined Server",
  #    "Group": "Servers",
  #    "Port": 5432,
  #    "Username": "postgres",
  #    "Host": "localhost",
  #    "SSLMode": "prefer",
  #    "MaintenanceDB": "postgres"
  #  }

networkPolicy:
  enabled: true

ingress:
  ## If true, pgAdmin4 Ingress will be created
  ##
  enabled: false

  ## pgAdmin4 Ingress annotations
  ##
  annotations: {}
    ## Indicate that the ingress should be handled by NGINX controller
    # kubernetes.io/ingress.class: nginx
    #
    ## When setting `ingress.hosts.paths`, pgAdmin needs additional header
    ## to be passed.
    ## Ref: https://www.pgadmin.org/docs/pgadmin4/latest/container_deployment.html#http-via-nginx
    # nginx.ingress.kubernetes.io/configuration-snippet: |
    #   proxy_set_header X-Script-Name /pgadmin4;
    #
    ## If TLS is terminated elsewhere (on external load balancer), you may want
    ## to redirect to `https://` URL scheme along with rewriting URL path if
    ## `ingress.hosts.paths` is set. This is specific for image version >= 4.22.
    ## Ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#proxy-redirect
    # nginx.ingress.kubernetes.io/proxy-redirect-from: "~^http://([^/]+)/(pgadmin4/)?(.*)$"
    # nginx.ingress.kubernetes.io/proxy-redirect-to: "https://$1/pgadmin4/$3"
    #
    ## Secure Ingress with kube-lego or cert-manager if they are deployed into
    ## the cluster.
    ## Ref: https://cert-manager.io/docs/usage/ingress/#optional-configuration
    # kubernetes.io/tls-acme: "true"

  ## pgAdmin4 Ingress hostnames with optional path
  ## Must be provided if Ingress is enabled
  hosts:
    - host: chart-example.local
      paths: []
        # - /pgadmin4

  ## pgAdmin4 Ingress TLS configuration
  ## Secrets must be manually created in the namespace
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Additional config maps to be mounted inside a container
# Can be used to map config maps for sidecar as well
extraConfigmapMounts: []
  # - name: certs-configmap
  #   mountPath: /etc/ssl/certs
  #   subPath: ca-certificates.crt # (optional)
  #   configMap: certs-configmap
  #   readOnly: true

extraSecretMounts: []
  # - name: pgpassfile
  #   secret: pgpassfile
  #   subPath: pgpassfile
  #   mountPath: "/var/lib/pgadmin/storage/pgadmin/file.pgpass"
  #   readOnly: true

## Specify additional containers in extraContainers.
## For example, to add an authentication proxy to a pgadmin4 pod.
extraContainers: |
# - name: proxy
#   image: quay.io/gambol99/keycloak-proxy:latest
#   args:
#   - -provider=github
#   - -client-id=
#   - -client-secret=
#   - -github-org=<ORG_NAME>
#   - -email-domain=*
#   - -cookie-secret=
#   - -http-address=http://0.0.0.0:4181
#   - -upstream-url=http://127.0.0.1:3000
#   ports:
#     - name: proxy-web
#       containerPort: 4181

## Provide the name for an existing secret.
## Useful to avoid specifying password and server config in YAML files
existingSecret: ""

## pgAdmin4 startup configuration
## Values in here get injected as environment variables
## Needed chart reinstall for apply changes
env:
  # can be email or nickname
  email: chart@example.local
  password: SuperSecret
  # pgpassfile: /var/lib/pgadmin/storage/pgadmin/file.pgpass

  # set context path for application (e.g. /pgadmin4/*)
  # contextPath: /pgadmin4

  ## If True, allows pgAdmin4 to create session cookies based on IP address
  ## Ref: https://www.pgadmin.org/docs/pgadmin4/latest/config_py.html
  ##
  enhanced_cookie_protection: "False"

  ## Add custom environment variables that will be injected to deployment
  ## Ref: https://www.pgadmin.org/docs/pgadmin4/latest/container_deployment.html
  ##
  variables: []
  # - name: PGADMIN_LISTEN_ADDRESS
  #   value: "0.0.0.0"
  # - name: PGADMIN_LISTEN_PORT
  #   value: "8080"

persistentVolume:
  ## If true, pgAdmin4 will create/use a Persistent Volume Claim
  ## If false, use emptyDir
  ##
  enabled: true

  ## pgAdmin4 Persistent Volume Claim annotations
  ##
  annotations: {}

  ## pgAdmin4 Persistent Volume access modes
  ## Must match those of existing PV or dynamic provisioner
  ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  accessModes:
    - ReadWriteOnce

  ## pgAdmin4 Persistent Volume Size
  ##
  size: 10Gi

  ## pgAdmin4 Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  # existingClaim: ""

## Security context to be added to pgAdmin4 pods
##
securityContext:
  runAsUser: 5050
  runAsGroup: 5050
  fsGroup: 5050

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

## pgAdmin4 readiness and liveness probe initial delay and timeout
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
livenessProbe:
  initialDelaySeconds: 30
  periodSeconds: 60
  timeoutSeconds: 15
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  initialDelaySeconds: 30
  periodSeconds: 60
  timeoutSeconds: 15
  successThreshold: 1
  failureThreshold: 3

## Required to be enabled pre pgAdmin4 4.16 release, to set the ACL on /var/lib/pgadmin.
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
##
VolumePermissions:
  ## If true, enables an InitContainer to set permissions on /var/lib/pgadmin.
  ##
  enabled: false

## Additional InitContainers to initialize the pod
##
extraInitContainers: |
#   - name: add-folder-for-pgpass
#     image: "dpage/pgadmin4:4.23"
#     command: ["/bin/mkdir", "-p", "/var/lib/pgadmin/storage/pgadmin"]
#     volumeMounts:
#       - name: pgadmin-data
#         mountPath: /var/lib/pgadmin
#     securityContext:
#       runAsUser: 5050


## Node labels for pgAdmin4 pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## Node tolerations for server scheduling to nodes with taints
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
##
tolerations: []

## Pod affinity
##
affinity: {}

## Pod annotations
##
podAnnotations: {}

init:
  ## Init container resources
  ##
  resources: {}

## Define values for chart tests
test:
  ## Container image for test-connection.yaml
  image:
    registry: docker.io
    repository: busybox
    tag: latest
  ## Resources request/limit for test-connection Pod
  resources: {}
#     limits:
#       cpu: 25m
#       memory: 16Mi
#     requests:
#       cpu: 50m
#       memory: 32Mi
  ## Security context for test-connection Pod
  securityContext:
    runAsUser: 5051
    runAsGroup: 5051
    fsGroup: 5051
